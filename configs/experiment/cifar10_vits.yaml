# @package _global_
defaults:
  - /task: pretrain
  - /model: vit_torchvision
  - /predictor: base
  - /data: cifar10
  - /masking: block
  - /loss: vanilla
  - /optim: adamw
  - /sched: warmup_cosine
  - /train: pretrain
  - _self_

model:
  arch: vit_small_16
  image_size: 32
  patch_size: 8
  embed_dim: 384
  num_heads: 6
  depth: 8
  ema_momentum: [0.996, 0.996]

data:
  root: ${oc.env:CIFAR10_ROOT,/scratch/ijepa_lite/data}
  batch_size: 256
  num_workers: 4

optim:
  lr: 3e-4
  weight_decay: 0.05

train:
  epochs: 10
  log_every: 50
  save_every_epochs: 1
  amp: true